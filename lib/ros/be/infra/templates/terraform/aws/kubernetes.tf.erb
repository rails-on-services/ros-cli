# Providers
provider "aws" {
  region  = "ap-southeast-1"
  profile = local.aws_profile
}

provider "kubernetes" {
  version                = "~> 1.9"
  host                   = module.eks-cluster.cluster_endpoint #data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(module.eks-cluster.cluster_certificate_authority_data) #base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data)
  token                  = data.aws_eks_cluster_auth.cluster-auth.token
  load_config_file       = false
}

provider "helm" {
  version         = "~> 0.10"
  namespace       = "kube-system"
  install_tiller  = true
  tiller_image    = "gcr.io/kubernetes-helm/tiller:v2.14.3"
  service_account = "tiller"

  kubernetes {
    host                   = module.eks-cluster.cluster_endpoint #data.aws_eks_cluster.cluster.endpoint
    cluster_ca_certificate = base64decode(module.eks-cluster.cluster_certificate_authority_data) #base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data)
    token                  = data.aws_eks_cluster_auth.cluster-auth.token
  }
}

provider "local" {
  version = "~> 1.3"
}

provider "null" {
  version = "~> 2.1"
}

provider "random" {
  version = "~> 2.2"
}

provider "template" {
  version = "~> 2.1"
}

# Locals
locals {
  aws_profile                         = "<%= tf.kubernetes.config.aws_profile.nil? ? "default" : tf.kubernetes.config.aws_profile %>"
  vpc_cidr                            = "<%= tf.vpc.config.cidr.nil? ? "10.0.0.0/16" : tf.vpc.config.cidr %>"
  vpc_create_database_subnet_group    = <%= tf.vpc.config.create_database_subnet_group.nil? ? false : true %>
  vpc_create_redshift_subnet_group    = <%= tf.vpc.config.create_redshift_subnet_group.nil? ? false : true %>
  vpc_create_elasticache_subnet_group = <%= tf.vpc.config.create_elasticache_subnet_group.nil? ? false : true %>
  vpc_name                            = join("-", ["stack", "<%= tf.kubernetes.config.name.nil? ? 'terraform.workspace' : tf.kubernetes.config.name %>"])
  cluster_name                        = join("-", ["stack", "<%= tf.kubernetes.config.name.nil? ? 'terraform.workspace' : tf.kubernetes.config.name %>"])
  accelerator_name                    = join("-", ["stack", "<%= tf.kubernetes.config.name.nil? ? 'terraform.workspace' : tf.kubernetes.config.name %>"])
  environment                         = "<%= tf.kubernetes.config.name.nil? ? 'terraform.workspace' : tf.kubernetes.config.name %>"
}

# VPC
data "aws_availability_zones" "available" {}

module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "~> 2.9.0"

  name = local.vpc_name 
  cidr = local.vpc_cidr

  enable_nat_gateway              = true
  enable_dns_hostnames            = true
  enable_dns_support              = true
  enable_s3_endpoint              = true
  create_database_subnet_group    = local.vpc_create_database_subnet_group
  create_redshift_subnet_group    = local.vpc_create_redshift_subnet_group
  create_elasticache_subnet_group = local.vpc_create_elasticache_subnet_group

  azs                 = slice(data.aws_availability_zones.available.names, 0, 3)
  public_subnets      = [for i in range(1, 4) : cidrsubnet(local.vpc_cidr, 8, i)] 
  private_subnets     = [for i in range(11, 14) : cidrsubnet(local.vpc_cidr, 8, i)]

  tags = {
    "Environment" = local.environment
  }

  vpc_tags = {
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
  }

  public_subnet_tags = {
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
    "kubernetes.io/role/elb"                          = "1"
  }

  private_subnet_tags = {
    "kubernetes.io/cluster/${local.cluster_name}" = "shared"
    "kubernetes.io/role/internal-elb"                 = "1"
  }
}

# IAM
module "iam" {
  source = "./aws/eks/modules/iam"
}

<% if tf.kubernetes.config.worker_groups.key_name.nil? || tf.kubernetes.config.worker_groups.key_name.empty? -%>
# AWS key pair
resource "aws_key_pair" "this" {
  key_name   = local.cluster_name
  public_key = file("~/.ssh/id_rsa.pub")
}
<% end -%>

# EKS cluster
module "eks-cluster" {
  source                    = "./aws/eks/modules/eks-cluster"
  cluster_name              = local.cluster_name
  vpc_id                    = module.vpc.vpc_id
  public_subnets            = module.vpc.public_subnets
  private_subnets           = module.vpc.private_subnets
  default_security_group_id = module.vpc.default_security_group_id
  aws_profile               = local.aws_profile

  eks_worker_groups = {
<% unless tf.kubernetes.config.worker_groups.nil? -%>
<% tf.kubernetes.config.worker_groups.each do |k, v| -%>
<% unless v.nil? || v.is_a?(String) && v.empty? -%>
    <%= k %> = <% if v.is_a?(String) %>"<%= v %>"<% else %><%= v %><% end %> 
<% end -%>
<% end -%>
<% if tf.kubernetes.config.worker_groups.key_name.nil? || tf.kubernetes.config.worker_groups.key_name.empty? -%>
    key_name = aws_key_pair.this.key_name
<% end -%>
<% end -%>
  }  

  eks_map_users = []
  eks_map_roles = module.iam.eks_map_roles
  
  tags = {
    "Environment" = local.environment
  }
}

<% unless tf.globalaccelerator.nil? || tf.globalaccelerator.config.nil? || ! tf.globalaccelerator.config.enabled -%>
# Global accelerator
module "globalaccelerator" {
  source                      = "./aws/eks/modules/globalaccelerator"
  name                        = local.accelerator_name
  route53_zone_id             = "<%= tf.globalaccelerator.config.route53_zone_id.nil? || tf.globalaccelerator.config.route53_zone_id.empty? ? '' : tf.globalaccelerator.config.route53_zone_id %>"
  global_accelerator_hostname = "<%= tf.globalaccelerator.config.hostname.nil? || tf.globalaccelerator.config.hostname.empty? ? '' : tf.globalaccelerator.config.hostname %>"
}

output "globalaccelerator_ips" {
  value = module.globalaccelerator.globalaccelerator_ips
}
<% end -%>

# EKS resources

#data "aws_eks_cluster" "cluster" {
#  name = local.cluster_name
#}

data "aws_eks_cluster_auth" "cluster-auth" {
  name = local.cluster_name
}

module "eks-resources" {
  source                     = "./aws/eks/modules/eks-resources"
  cluster_name               = local.cluster_name
  extra_namespaces           = []
  clusterrolebindings        = module.iam.kubernetes_clusterrolebindings
  perx_helm_server_username  = "<%= tf.kubernetes.config.cluster_resources.perx_helm_server_username %>"
  perx_helm_server_password  = "<%= tf.kubernetes.config.cluster_resources.perx_helm_server_password %>"
  enable_external_dns        = <%= tf.kubernetes.config.cluster_resources.enable_external_dns %>
  external_dns_domainFilters = <%= tf.kubernetes.config.cluster_resources.external_dns_domainFilters %>
  external_dns_zoneIdFilters = <%= tf.kubernetes.config.cluster_resources.external_dns_zoneIdFilters %>
  istio_version              = "<%= tf.kubernetes.config.cluster_resources.istio_version %>"

  istio_ingressgateway_alb_cert_arn                 = "" #local.aws_acm_cert
  enable_fluentd_gcp_logging                        = false
  #fluentd_gcp_logging_service_account_json_key_path = var.fluentd_gcp_logging_service_account_json_key_path
  kubeconfig = module.eks-cluster.kubeconfig
}

output "cluster_endpoint" {
  value = module.eks-cluster.cluster_endpoint
}

output "cluster_certificate_authority_data" {
  value = module.eks-cluster.cluster_certificate_authority_data
}

output "eks" {
  value     = module.eks-cluster.this
  sensitive = true
}

output "vpc" {
  value     = module.vpc.*
  sensitive = true
}

output "iam" {
  value     = module.iam.*
  sensitive = true
}

