apiVersion: batch/v1
kind: Job
metadata:
  name: kafka-connect-connector-provision
spec:
  template:
    metadata:
      name: kafka-connect-connector-provision
      annotations:
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: OnFailure
      containers:
        <%- @service.kafka_connect.connectors.each do |name, config| -%>
        <%- if config.type == 'bigquery' -%>
        <%- @service.cloudevents_subjects.each do |svc, topics| -%>
        - image: gcr.io/cloud-builders/curl
          name: <%= name %>-<%= svc.to_s %>
          command:
            - curl
            - -X
            - PUT
            - http://kafka-connect-<%= @service.current_feature_set %>:8083/connectors/<%= name %>-<%= @service.current_feature_set %>-<%= svc.to_s %>/config
            - -H
            - "Content-Type: application/json"
            - -H
            - "Accept: application/json"
            - -d
            - >
              {
                "connector.class": "com.wepay.kafka.connect.bigquery.BigQuerySinkConnector",
                "autoUpdateSchemas": "true",
                "bigQueryMessageTimePartitioning": "false",
                "autoCreateTables": "true",
                "sanitizeTopics": "true",
                "tasks.max": "<%= config.tasks ? config.tasks : 2 %>",
                "topics": "<%= topics.join(',') %>",
                "schemaRegistryLocation": "http://kafka-schema-registry:8081",
                "topicsToTables": "(\\w+)\\.(\\w+)=$1_$2",
                "project": "<%= config.project %>",
                "datasets": ".*=<%= @service.bigquery_dataset %>",
                "keyfile": "/etc/google/auth/application_default_credentials.json",
                "schemaRetriever": "com.wepay.kafka.connect.bigquery.schemaregistry.schemaretriever.SchemaRegistrySchemaRetriever",
                "key.converter": "org.apache.kafka.connect.storage.StringConverter",
                "errors.tolerance": "all",
                "errors.log.include.messages": "true",
                "errors.log.enable": "true"
              }
        <%- end -%>
        <%- end -%>
        <%- if config.type == 'bigquery' -%>
        - image: google/cloud-sdk:alpine
          name: <%= name %>-create-bq-dataset
          env:
            - name: GOOGLE_APPLICATION_CREDENTIALS
              value: /etc/google/auth/application_default_credentials.json
          command:
            - sh
            - -c
            - |
              gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}
              exists=$(bq --project_id <%= config.project %> ls -d | grep -w <%= @service.bigquery_dataset %>)
              if [ -n "$exists" ]; then
                echo "Not creating <%= @service.bigquery_dataset %> since it already exists"
              else
                echo "Creating <%= @service.bigquery_dataset %>"
                bq --project_id <%= config.project %> mk <%= @service.bigquery_dataset %>
              fi
          volumeMounts:
            - name: gcp-jsonkey
              mountPath: /etc/google/auth
        <%- end -%>
        <%- end -%>
      volumes:
        - name: gcp-jsonkey
          secret:
            defaultMode: 420
            secretName: gcp-jsonkey
            optional: true
